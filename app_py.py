# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DEvCC-2VhP-wtYyADBGSt719V5IRkRNG
"""

#import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

true_news = pd.read_csv('/content/drive/MyDrive/True.csv', encoding = 'latin-1', on_bad_lines = 'skip')
fake_news = pd.read_csv('/content/drive/MyDrive/Fake.csv', encoding ='latin-1', on_bad_lines ='skip')

true_news.shape, fake_news.shape

true_news.info()

fake_news.info()

true_news.describe()

fake_news.describe()

true_news["class"] = 1
fake_news["class"] = 0

fake_news_manual_testing = fake_news.tail(10)
fake_news = fake_news.drop(fake_news.tail(10).index, inplace=False)


true_news_manual_testing = true_news.tail(10)
true_news = true_news.drop(true_news.tail(10).index, inplace=False)

true_news_manual_testing['class'] = 1
fake_news_manual_testing['class'] = 0

true_news_manual_testing.head()

fake_news_manual_testing.tail()

data_merge = pd.concat([true_news, fake_news], axis=0)

data_merge.info()

data_merge.describe()

data_merge.shape

data_merge.isnull().sum()

data_merge.columns

data = data_merge.drop(['title', 'subject', 'date'], axis=1)
data.head()

data = data.sample(frac=1)
data.head()

data.reset_index(inplace=True)
data.drop(['index'], axis=1, inplace=True)

data.columns

data.head()

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)  # Remove special characters
    text = text.split()
    text = [word for word in text if word not in stop_words]  # Remove stopwords
    text = [lemmatizer.lemmatize(word) for word in text]  # Lemmatization
    return ' '.join(text)

data['cleaned_text'] = data['text'].apply(preprocess_text)

# Train-Test split
X = data['cleaned_text']
y = data['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorization
vectorizer = TfidfVectorizer()
X_train_vector = vectorizer.fit_transform(X_train)
X_test_vector = vectorizer.transform(X_test)

sns.countplot(x='class', data=data, palette=['red', 'green'])
plt.title('Class Distribution Fake VS Real')
plt.show()                                # (1==True News , 0==Fake News)

#Sentiment analysis
from textblob import TextBlob

def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

data['sentiment'] = data['cleaned_text'].apply(get_sentiment)

# Visualize Sentiment
plt.figure(figsize=(10, 6))
sns.histplot(data['sentiment'], bins=20, kde=True)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

#N-gram Analysis
from sklearn.feature_extraction.text import CountVectorizer

def plot_ngrams(n, data):
  vectorizer = CountVectorizer(ngram_range=(n, n), max_features=20)
  ngrams = vectorizer.fit_transform(data)
  ngram_counts = ngrams.sum(axis=0).A1
  ngram_features = vectorizer.get_feature_names_out()

  ngram_df = pd.DataFrame({'ngram': ngram_features, 'count': ngram_counts})
  ngram_df = ngram_df.sort_values(by='count', ascending=False)

  plt.figure(figsize=(10, 6))
  sns.barplot(x='count', y='ngram', data=ngram_df, palette='viridis')
  plt.title(f'Top {n}-grams')
  plt.show()

plot_ngrams(1, data['cleaned_text'])

plot_ngrams(2, data['cleaned_text'])

plot_ngrams(3, data['cleaned_text'])

# wordCloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# WordCloud For Real News
real_text = ' '.join(data[data['class'] == 1]['cleaned_text'])
wordcloud = WordCloud(width=800, height=400).generate(real_text)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud for Real News')
plt.axis('off')
plt.show()

# WordCloud for Fake News
fake_text = ' '.join(data[data['class'] == 0]['cleaned_text'])
wordcloud = WordCloud(width=800, height=400).generate(fake_text)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud for Fake News')
plt.axis('off')
plt.show()

from sklearn.model_selection import GridSearchCV

# X = data['cleaned_text']
# y = data['class']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# vectorizer = TfidfVectorizer()
# X_train_vector = vectorizer.fit_transform(X_train)
# X_test_vector = vectorizer.transform(X_test)

# Logistic Regression tuning
param_grid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
grid_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')
grid_lr.fit(X_train_vector, y_train)

pred_lr = grid_lr.predict(X_test_vector)

grid_lr.score(X_test_vector, y_test)

print(classification_report(y_test, pred_lr))

# Random Forest Classifier
from sklearn.model_selection import RandomizedSearchCV

param_grid_rf = {'n_estimators': [100], 'max_depth': [None, 50, 100]}

grid_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_grid_rf, n_iter=5,
                             cv=3, scoring='accuracy', n_jobs=-1, random_state=42, verbose=2)
grid_rf.fit(X_train_vector, y_train)

print("Best Parameters:", grid_rf.best_params_)

pred_rf = grid_rf.predict(X_test_vector)

grid_rf.score(X_test_vector, y_test)

print(classification_report(y_test, pred_rf))

# svm tuning
grid_svm = SVC(kernel='linear')
grid_svm.fit(X_train_vector, y_train)

pred_svm = grid_svm.predict(X_test_vector)

grid_svm.score(X_test_vector, y_test)

print(classification_report(y_test, pred_svm))

# DecisionTree Classifier
from sklearn.tree import DecisionTreeClassifier

grid_dt = DecisionTreeClassifier()
grid_dt.fit(X_train_vector, y_train)

pred_dt = grid_dt.predict(X_test_vector)

grid_dt.score(X_test_vector, y_test)

print(classification_report(y_test, pred_dt))

# Gradient Boost Classifier
from sklearn.ensemble import GradientBoostingClassifier

grid_gb = GradientBoostingClassifier(random_state = 0)
grid_gb.fit(X_train_vector, y_train)

pred_gb = grid_gb.predict(X_test_vector)

grid_gb.score(X_test_vector, y_test)

print(classification_report(y_test, pred_gb))

best_lr = grid_lr.best_estimator_

for model, name in [(best_lr, 'Logistic Regression')]:
    predictions = model.predict(X_test_vector)
    print(f"Model: {name}")
    print(f"Accuracy: {accuracy_score(y_test, predictions)}")
    print(f"Classification Report:\n{classification_report(y_test, predictions)}")
    sns.heatmap(confusion_matrix(y_test, predictions), annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for Logistic Regression model')
    plt.show()

best_rf = grid_rf.best_estimator_

for model, name in [(best_lr, 'RandomForest Classifier')]:
    predictions = model.predict(X_test_vector)
    print(f"Model: {name}")
    print(f"Accuracy: {accuracy_score(y_test, predictions)}")
    print(f"Classification Report:\n{classification_report(y_test, predictions)}")
    sns.heatmap(confusion_matrix(y_test, predictions), annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for Randomforest Classifier')
    plt.show()

cm = confusion_matrix(y_test, pred_svm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.title('Confusion Matrix for SVM (Linear Kernel)')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

Dt = confusion_matrix(y_test, pred_dt)

plt.figure(figsize=(8, 6))
sns.heatmap(Dt, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.title('Confusion Matrix for decisiontree classifier')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

Gb = confusion_matrix(y_test, pred_gb)

plt.figure(figsize=(8, 6))
sns.heatmap(Gb, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.title('Confusion Matrix for Gradient boost classifier')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

# Comparison between the models
accuracy_lr = accuracy_score(y_test, pred_lr)
accuracy_rf = accuracy_score(y_test, pred_rf)
accuracy_svm = accuracy_score(y_test, pred_svm)
accuracy_dt = accuracy_score(y_test, pred_dt)
accuracy_gb = accuracy_score(y_test, pred_gb)

models = ['Logistic Regression', 'Random Forest', 'SVM', 'Decision Tree', 'Gradient Boost']
accuracies = [accuracy_lr, accuracy_rf, accuracy_svm, accuracy_dt, accuracy_gb]

plt.figure(figsize=(10, 6))
sns.barplot(x=models, y=accuracies, palette=['blue', 'green', 'red', 'yellow', 'violet'])
plt.title('Model Comparison - Accuracy')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.show()

# roc curve visualization
from sklearn.metrics import roc_curve, roc_auc_score

def plot_roc_curve(y_test, y_pred_prob, model_name):
  fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
  auc_score = roc_auc_score(y_test, y_pred_prob)

  plt.figure(figsize=(8, 6))
  plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.2f})')
  plt.plot([0, 1], [0, 1], 'k--')
  plt.title(f'ROC Curve for {model_name}')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.legend()
  plt.show()

# For Logistic Regression
y_pred_prob_lr = grid_lr.predict_proba(X_test_vector)[:, 1]
plot_roc_curve(y_test, y_pred_prob_lr, 'Logistic Regression')

# For Random Forest
y_pred_prob_rf = grid_rf.predict_proba(X_test_vector)[:, 1]
plot_roc_curve(y_test, y_pred_prob_rf, 'Random Forest')

# For svm
y_pred_prob_svm = grid_svm.decision_function(X_test_vector)
plot_roc_curve(y_test, y_pred_prob_svm, 'SVM')

# For Decision Tree
y_pred_prob_dt = grid_dt.predict_proba(X_test_vector)[:, 1]
plot_roc_curve(y_test, y_pred_prob_dt, 'Decision Tree')

# For Gradient Boost Classifier
y_pred_prob_gb = grid_gb.predict_proba(X_test_vector)[:, 1]
plot_roc_curve(y_test, y_pred_prob_gb, 'Gradient Boost classifier')

# Precision Recall Curve
from sklearn.metrics import precision_recall_curve

def plot_precision_recall_curve(y_test, y_pred_prob, model_name):
  precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)

  plt.figure(figsize=(8, 6))
  plt.plot(recall, precision, label=f'{model_name}')
  plt.title(f'Precision-Recall Curve for {model_name}')
  plt.xlabel('Recall')
  plt.ylabel('Precision')
  plt.legend()
  plt.show

plot_precision_recall_curve(y_test, y_pred_prob_lr, 'Logistic Regression')
plot_precision_recall_curve(y_test, y_pred_prob_rf, 'Random Forest')
plot_precision_recall_curve(y_test, y_pred_prob_svm, 'SVM')
plot_precision_recall_curve(y_test, y_pred_prob_dt, 'Decision Tree')
plot_precision_recall_curve(y_test, y_pred_prob_gb, 'Gradient Boost Classifier')

# Feature Importance visualization for random forest
feature_importances = grid_rf.best_estimator_.feature_importances_
features = vectorizer.get_feature_names_out()

feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False).head(20)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('Top 20 Important Features (Random Forest)')
plt.show()

def output_lable(n):
    if n == 0:
        return "Fake News"
    elif n == 1:
        return "Real News"

def manual_testing(news):
    testing_news = {"text": [news]}
    new_def_test = pd.DataFrame(testing_news)
    new_def_test["cleaned_text"] = new_def_test["text"].apply(preprocess_text)
    new_x_test = new_def_test["cleaned_text"]
    new_x_test_vector = vectorizer.transform(new_x_test)
    pred_LR = grid_lr.predict(new_x_test_vector)
    pred_DT = grid_dt.predict(new_x_test_vector)
    pred_GB = grid_gb.predict(new_x_test_vector)
    pred_RF = grid_rf.predict(new_x_test_vector)
    pred_SVM = grid_svm.predict(new_x_test_vector)

    return print("\n\nLR Prediction: {} \nDT Prediction: {} \nGB Prediction: {} \nRF Prediction: {} \nSVM Prediction: {}".format(output_lable(pred_LR[0]),
                                                                                                                                 output_lable(pred_DT[0]),
                                                                                                                              output_lable(pred_GB[0]),
                                                                                                                              output_lable(pred_RF[0]),
                                                                                                                              output_lable(pred_SVM[0])))

data['cleaned_text'][20]

news = str(input())                   # Example of Real News
manual_testing(news)

data['cleaned_text'][200]

news = str(input())                    # Example of fake news
manual_testing(news)

!pip install streamlit
import streamlit as st
import joblib
import pandas as pd

joblib.dump(grid_lr, 'logistic_regression_model.joblib')
joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')

# Load trained model and vectorizer
model = joblib.load('logistic_regression_model.joblib')
vectorizer = joblib.load('tfidf_vectorizer.joblib')

# Title and instructions
st.title('Fake News Detection')
st.write("Enter the news article text below to classify whether it is 'Real' or 'Fake'.")

# Text input from the user
news_text = st.text_area("News Article Text", height=300)

# Prediction logic when user clicks "Classify"
if st.button("Classify"):
    if news_text:  # If there's text input
        # Transform the input text using the vectorizer
        transformed_text = vectorizer.transform([news_text])

        # Make a prediction using the loaded model
        prediction = model.predict(transformed_text)
        result = 'Fake' if prediction[0] == 0 else 'Real'

        # Display the result inside the if st.button("Classify"): block
        st.success(f'The news is **{result}**.') # Moved inside the 'if' block
    else:
        st.error("Please enter some text to classify.")

!streamlit run app.py